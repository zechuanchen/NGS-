# 16S 生物信息分析上（2）

标签（空格分隔）： 杨思佳

---

## **基本分析流程：**
![image_1e5tm6b6i190p1p4n1tjk8p815bc9.png-158.4kB][1]
先从公司pdf上copy的一段话：
原始数据 Raw data下机后先将低**质量 reads 过滤掉**，得到的有效数据 clean data 用于后续分析。Reads 间通过
**overlap 拼接成 Tags. 并且在特定的相似度下将 Tags 聚类成 OTU，基于OTU 聚类结果可以对 OTU 进行多种多样性**分析，及测序深度检测。再通过 OTU 与数据库比对，对OTU 进行分类注释， 基于分类学信息，可以在各个分类水平上进行群落结构的统计分析。基于上述结果，进一步对群落结构和系统发育等进行深入统计分析和可视化分析，及功能预测分析。

**先简单说一下主要流程：拼接reads，质量过滤（这两步可以颠倒顺序），去重，去嵌合体，OTU聚类，统计每个样本OTU数量，OTU注释，OTU表格下游分析。**


## **宏基因组常用软件平台介绍：**
![image_1e5tm9siq7cq1ns812o1112r1pqn1g.png-398.1kB][2]

### **Mothur（2009）**
整合了200多个软件，有完整的16s分析SOP，数据分析比较直观，Galaxy在线分析平台能直接调用分析，适合没有编程基础的人学习，但是不适合通用于代码分析（主要是交互形）。
因为我们的肠道菌群数据有reads重复的问题， 之后去处reads重复问题之后，在比对到Mothur特制的silva数据库的时候，比对失败（不知道原因= =!）。因此这次分析没有用Mothur流程。

### **Qiime1（2010）, Qiime2（2019）**
Qiime2刚出，**颠覆了OTU的概念，改为feature**。因为太新，没有很多教程。所以这次选择的是传统qiime1，qiime1有200多个小软件，并且结合了Mothur，Vsearch， Usearch的一部分功能。

### **Usearch（2010）, Vsearch（2016）**
Usearch序列比对快，并且包括UCHIME算法去除嵌合体，还有UPARSE是OTU聚类的金标准，UNOISE方法去除illumina测序噪音。
Vsearch： 免费版本的Usearch，能替代Usearch一部分的功能。


## **此次分析的软件(都是conda安装）：**
质控：FastQC (v0.11.9)
去接头过滤：cutadapt(2.6)
fastq转fasta: bioawk (20110810)
拼接，过滤，选择OTU: vsearch (2.4.4)
注释：Qiime1.9.1 （要求python2.7，建立qiime1环境）

```bash
conda create -n qiime1 python=2.7 qiime matplotlib=1.4.3 mock nose -c bioconda
```

后续做图多样性分析：R version 3.6.1 (2019-07-05)
R包：vegan, phyloseq, 以及其他常用绘图包pheatmap, Rcolorbrewer, 数据整理包tidyverse等

## 具体分析流程：

参考：
[http://blog.sciencenet.cn/blog-3334560-1071618.html](http://blog.sciencenet.cn/blog-3334560-1071618.html)（宏基因组-扩增子分析流程 中科大 刘永鑫）
[https://github.com/torognes/vsearch/wiki/VSEARCH-pipeline](https://github.com/torognes/vsearch/wiki/VSEARCH-pipeline)（VSEARCH Miseq SOP）

## **1. 提取barcode，样品拆分**
[https://mp.weixin.qq.com/s/S5_BmKlbVhKL-qZf6V69Xw?](https://mp.weixin.qq.com/s/S5_BmKlbVhKL-qZf6V69Xw?)
如果你的数据是原始数据带有barcode，请参考这个教程先产生mapping fille, 然后运用qiime的extract_barcode.py 函数进行barcode提取和样品拆分。

## **2. 质控，切除扩增引物 (cutadapt 2.6)**
由公司报告得出：

- 正向引物
(5’- GTGCCAGCMGCCGCGGTAA -3’)
- 反向引物806R
(5’-GGACTACHVGGGTWTCTAAT-3’)

用cutadapt切除双端引物：<br />

```bash
cutadapt --cores=8 -a GTGCCAGCMGCCGCGGTAA  \
	-e 0.2  ${id}_1.fastq -o ${OUT}/${id}_1.filtered.fastq \
  2> ${OUT}/${id}_1.cutadpt.log

cutadapt --cores=8 -a ATTAGAWACCCBDGTAGTCC \
	-e 0.2  ${id}_2.fastq -o ${OUT}/${id}_2.filtered.fastq \
  2> ${OUT}/${id}_2.cutadpt.log
```
参数：
-a 3’端引物
-e 引物匹配允许错误率，调置0.20，一般引物20bp长允许3-4个错配，为了尽量把引物切干净
--cores=8 cpu数为8

## **3. 拼接双端序列 (vsearch 2.4.4)**
由于公司给我的数据是已经是根据barcode把样品拆分好的，这里我就直接进行拼接了。由于公司的数据虽然是双端150，但是一对双端序列重合bp比较少，所以我这里跟公司分析选择参数一样：最小重合数为5就进行拼接，而且只能使用vsearch2.4.4版本，因为之后的版本最小的拼接长度为8bp重合。


```bash
vsearch --fastq_mergepairs ${id}_1.fastq --reverse ${id}_2.fastq \
	--fastq_minovlen 5 --fastqout ${OUT}/${id}.merged.fastq 2> ${OUT}/log/${id}.merged.log 
```
参数：
--fastq_minovlen 5 最小重叠数
--input raw fastq reads
--output merged.fastq


## **4. 质控，长度过滤 (cutadapt 2.6)，转换fasta格式 (bioawk 20110810)**


```bash
cutadapt --cores=8 -q 20 --max-n 0 -m 150 ${mergeOUT}/${id}.merged.fastq \
	> ${mergeOUT}/${id}.merged.cleaned.fastq 
```

-m 最小序列长度，根据情况设置，本实验扩增V4区，长度主要位于250，故去除长度小于150bp的序列
--max-n=0 不容许有N碱基的存在
-q 切除默认window平均测序质量小于20的碱基及其下游
因为我们已经对数据进行了质量过滤，所以这边就将fastq格式转换为fasta

```bash
echo "converting to clean fasta"
bioawk -c fastx '{print ">"$name "\n" $seq }' ${mergeOUT}/${id}.merged.cleaned.fastq \
	> ${fastaOUT}/${id}.cleaned.fasta
```


以下流程主要使用的是vsearch给的16s miseq SOP流程。
[https://github.com/torognes/vsearch/wiki/VSEARCH-pipeline](https://github.com/torognes/vsearch/wiki/VSEARCH-pipeline)
更多用法请参考：
[https://usermanual.wiki/Document/vsearchmanual.523130275/html](https://usermanual.wiki/Document/vsearchmanual.523130275/html)
中文版请参考：
[https://blog.csdn.net/woodcorpse/article/details/81447992](https://blog.csdn.net/woodcorpse/article/details/81447992)
[https://www.jianshu.com/p/4b6b83f8a9a9](https://www.jianshu.com/p/4b6b83f8a9a9)
## **5. 对所有样本的fasta进行去重，将fasta序列重命名为样本名，保留序列的count数，合并所有样本fasta文件**

```bash
for id in `ls *.cleaned.fasta`;
do echo ${id%%.*}
vsearch --derep_fulllength ${id%%.*}.cleaned.fasta \
	--strand plus --output ${id%%.*}.derep.fasta \
  --sizeout --relabel ${id%%.*}. --fasta_width 0
done

#cd 到OUT目录
cd ${OUT}
rm -f all.fasta all.derep.fasta all.nonchimeras.derep.fasta
#合并所有的derep.fa
cat ${fastaOUT}/*derep.fasta > ${OUT}/all.fasta 

```
参数：
--derep_fulllength fastafile
对样本进行去重，如果序列一样长度相同，则视为同一序列(T和U被视为同一个碱基)。
**--size out**
**在去重时加这个选项，对输出的fasta记录每一条序列的总共个数**
**--size in**
**读取该fasta文件时，读取每条reads的个数信息。**
--relabel
将每个reads标记该reads来源的样本
--strand plus
判定序列是否合并时，只考虑一个方向
--fasta_width 0
默认设置为80，设置output的fasta中，每行有多少个核苷酸。设置为0则将核苷酸排列为一行
```
cat file* > all.file
```
将所有样本的fasta文件进行合并成all.fasta，这些fasta都有read count信息，**之后步骤都是对all.fasta进行处理**

## 6. 去重复，去singleton，预聚类


```bash
#重复derep 去除singleton
vsearch --derep_fulllength all.fasta \
   --minuniquesize 2 \
   --sizein \
   --sizeout \
   --fasta_width 0 \
   --uc all.derep.uc \
   --output all.derep.fasta
   echo Unique non-singleton sequences: $(grep -c "^>" all.derep.fasta)
```
--minuniquesize: 
在去重之后统计各unique序列的丰度，小于该值则去除，此处设置为2去除singleton<br />singleton在此处指的是只有一个count的序列，这些序列大概率是因为测序错误，或者测序深度不够的rare species。

```bash
#按98%相似度预聚类
vsearch --cluster_size all.derep.fasta \
   --id 0.98 \
   --strand plus \
   --sizein \
   --sizeout \
   --fasta_width 0 \
   --uc all.preclustered.uc \
   --centroids all.preclustered.fasta
echo Unique sequences after preclustering: $(grep -c "^>" all.preclustered.fasta)
```
--cluster_size 按给定相似度阈值进行clustering，此处双端拼接的序列大概长为250bp，因此0.98的意思是允许5个错配碱基。（序列小于等于5个错配碱基的序列都会被合并到一起）
--id 相似度阈值
--uc 产生一个uc文件记录每个reads作为Hit（H）合并到中心，还是centroid（S）作为聚类中心。
--centroids 在cluster --sizeout中加入这个参数记录每个中心的reads数和序列


## **7.去嵌合体（包括denovo 和 reference based）**
嵌合体序列是PCR扩增时，两条不同的序列产生杂交、扩增的序列。
从denovo的角度去除嵌合体

```bash
#De novo chimera detection
echo De novo chimera detection
vsearch --uchime_denovo all.preclustered.fasta \
   --sizein \
   --sizeout \
   --fasta_width 0 \
   --nonchimeras all.denovo.nonchimeras.fasta
echo Unique sequences after de novo chimera detection: $(grep -c "^>" all.denovo.nonchimeras.fasta)
```

--nonchimeras 扔掉嵌合体序列，保留剩下的序列

参考数据库进行嵌合体去除（数据库下载方法放在备注）
```bash
#Reference chimera detection
echo Reference chimera detection
vsearch --uchime_ref all.denovo.nonchimeras.fasta \
   --db ${REF}/rdp_gold.fa \
   --sizein \
   --sizeout \
   --fasta_width 0 \
   --nonchimeras all.ref.nonchimeras.fasta
echo Unique sequences after reference-based chimera detection: $(grep -c "^>" all.ref.nonchimeras.fasta)
```
-db reference fasta file or db file


## **8.提取每个样本中的非singleton非嵌合体的unique序列**
注：这个perl脚本我看不懂，不会perl（T.T），只知道功能。（脚本在原vsearch教程里有，备注里也有）
```bash
perl map.pl all.derep.fasta all.preclustered.uc all.ref.nonchimeras.fasta\
	> all.nonchimeras.derep.fasta
echo Sum of unique non-chimeric, non-singleton sequences in each sample: $(grep -c "^>" all.nonchimeras.fasta)
```


## **9.OTU聚类（0.97相似度），产生OTU表**


```bash
echo Cluster at 97% and relabel with OTU_n, generate OTU table
vsearch --cluster_size all.nonchimeras.fasta \
   --id 0.97 \
   --strand plus \
   --sizein \
   --sizeout \
   --fasta_width 0 \
   --uc all.clustered.uc \
   --relabel OTU_ \
   --centroids all.otus.fasta \
   --otutabout all.otutab.txt
```
--otutabout 输出otu表结构的，每一列为sample，每一行为otu，值为sequence count
--centroids 输出中心代表序列，以及其read count


## **10.OTU表注释(qiime1.9.1, greengenes数据库)整合**
gg_13_8_otus 数据库是qiime自带的，可以通过print_qiime_config.py -t  命令寻找绝对路径。
例如我这边是：/biosoft/anaconda3/envs/qiime1/lib/python2.7/site-packages/qiime_default_reference/gg_13_8_otus/

去除非细菌序列（选做）
运用qimme自带align_seqs.py 把all.otus.fasta 和 greengenes数据库的细菌序列进行比对。


```bash
align_seqs.py -i all.otus.fasta  \
 -t gg_13_8_otus/rep_set/97_otus.fasta \
 -o aligned/
```
随后进入aligned文件夹用wc -l统计非细菌序列数量。（我的数据并没有出现非细菌序列）

运用qiime自带函数assign_taxonomy.py 进行OTU表注释。

```bash
assign_taxonomy.py -i all.otus.fasta  \
 -r gg_13_8_otus/rep_set/97_otus.fasta \
 -t gg_13_8_otus/taxonomy/97_otu_taxonomy.txt \
-o result
```
-i input otu table
-r reference fasta
-t outputfile
-o output dir
taxonomy.txt 格式，每个otu都会有对应的注释。

![image_1e5tnqrf434o8t3avfh40uom1t.png-118.1kB][3]

进入R，将taxonomy.txt 与otutab.txt 根据OTU编号进行合并，处理成在线分析网站能用的数据格式，可以比较方便的出图。（略）
回顾一下上述过程：

![image_1e5tnrq251eo81af41kep1tj91k6p2a.png-146.9kB][4]

![image_1e5tns6gisn814l1ld2nqb1tbd2n.png-211.1kB][5]

## 11.后续分析做图（包括alpha多样性，beta多样性等等）
可以用[https://www.microbiomeanalyst.ca/MicrobiomeAnalyst/](https://www.microbiomeanalyst.ca/MicrobiomeAnalyst/upload/OtuUploadView.xhtml)网站进行分析，需要提交otutable（包含）taxonomy信息，样品分组信息，以及选择注释的基因库（此处是greengenes）
示例：
![image_1e5tntnjqjom6v0k021t464g43h.png-24.4kB][6]

## **12. 备注**
1.reference db 下载（用于嵌合体去除）：
```bash
echo Obtaining Gold reference database for chimera detection

if [ ! -e gold.fasta ]; then

    if [ ! -e Silva.gold.bacteria.zip ]; then
        wget https://www.mothur.org/w/images/f/f1/Silva.gold.bacteria.zip
    fi

    echo Decompressing and reformatting...
    unzip -p Silva.gold.bacteria.zip silva.gold.align | \
        sed -e "s/[.-]//g" > gold.fasta

fi
```

man.pl脚本
```perl
#!/usr/bin/perl -w                                                              

use warnings;
use strict;

die "Three arguments needed: fasta1, uc, and fasta2\n" unless scalar @ARGV > 2;

my ($fasta1, $uc, $fasta2) = @ARGV;

# read fasta2 file with accepted sequences                                      

my %accepted = ();

open(F2, $fasta2);
while (<F2>)
{
    if (/^>([^ ;]+)/)
    {
     	$accepted{$1} = 1;
    }
}
close F2;

# read uc file with mapping                                                     

open(UC, $uc);
while (<UC>)
{
    chomp;
    my @col = split /\t/;

    my $a;
    if ($col[8] =~ /^([^ ;*]+)/)
    {
     	$a = $1;
    }

    my $b;
    if ($col[9] =~ /^([^ ;*]+)/)
    {
     	$b = $1;
    }

    if ((defined $b) && ($accepted{$b}) && (defined $a))
    {
        $accepted{$a} = 1;
    }
}
close UC;

# read original fasta1 file                                                     

my $ok = 0;
open(F1, $fasta1);
while (<F1>)
{
    if (/^>([^ ;]+)/)
    {
     	$ok = $accepted{$1};
    }
    print if $ok;
}
close F1;
```


  [1]: http://static.zybuluo.com/czc/ah1r41bv27snsh9a5p6zdkpm/image_1e5tm6b6i190p1p4n1tjk8p815bc9.png
  [2]: http://static.zybuluo.com/czc/38d2ikvdfhb1jvyksmlj3c1j/image_1e5tm9siq7cq1ns812o1112r1pqn1g.png
  [3]: http://static.zybuluo.com/czc/f3f3570wecpzl469eqrm98mu/image_1e5tnqrf434o8t3avfh40uom1t.png
  [4]: http://static.zybuluo.com/czc/cva2c5wkt4sl5ns2zum753y8/image_1e5tnrq251eo81af41kep1tj91k6p2a.png
  [5]: http://static.zybuluo.com/czc/dh2hdz4dc78yb9mxmo3kwunz/image_1e5tns6gisn814l1ld2nqb1tbd2n.png
  [6]: http://static.zybuluo.com/czc/40mcqihkqpxcdflf8zmhjr6g/image_1e5tntnjqjom6v0k021t464g43h.png